{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOYrVuS3389Zsxw5enwZn8T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 신경망 모델 구성하기\n","- URL from : [PyTorch Tutorial 신경망 모델 구성하기](https://tutorials.pytorch.kr/beginner/basics/buildmodel_tutorial.html)\n","- 신경망은 데이터에 대한 연산을 수행하는 계층(layer)과 모듈(module)로 구성되어 있다.\n","- torch.nn에서는 신경망을 구성하는데 필요한 구성 요소를 제공하고 있다. PyTorch의 모든 모듈은 nn.Module의 하위 클래스다.\n","- 신경망은 다른 모듈, 계층으로 구성된 모듈이다. 이러한 중첩된 구조는 복잡한 아키텍처를 쉽게 구축하고 관리할 수 있다."],"metadata":{"id":"omqvT3mVDWhS"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"MCJel_yADNz7","executionInfo":{"status":"ok","timestamp":1672555447776,"user_tz":-540,"elapsed":2895,"user":{"displayName":"이승윤","userId":"02154129095399496427"}}},"outputs":[],"source":["# 필요한 라이브러리를 불러오자\n","import os\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms"]},{"cell_type":"markdown","source":["학습을 위한 장치를 얻어보자. 가능한 경우엔 GPU와 같은 하드웨어 가속기에서 모델을 학습하는 게 유리하다. torch.cuda를 사용할 수 있는지 확인하고 안 되면 CPU를 계속 사용하게 된다."],"metadata":{"id":"qvVbMbyDETvj"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"{device}를 사용할 수 있습니다!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VlBRm7t7ESGd","executionInfo":{"status":"ok","timestamp":1672555448290,"user_tz":-540,"elapsed":5,"user":{"displayName":"이승윤","userId":"02154129095399496427"}},"outputId":"23eae900-3d75-4057-e069-bbd7aba89230"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda를 사용할 수 있습니다!\n"]}]},{"cell_type":"markdown","source":["클래스를 정의해보자\n","- 신경망 모델을 nn.Module의 하위 클래스로 정의하고, __init__에서 신경망 계층들을 초기화한다.\n","- nn.Module을 상속받은 모든 클래스는 forward 메소드에 입력 데이터에 대한 연산들을 구현한다."],"metadata":{"id":"CrBOvnmeFMaW"}},{"cell_type":"code","source":["class NeuralNetwork(nn.Module) :\n","    def __init__(self) :\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10)\n","        )\n","    \n","    def forward(self, x) :\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits"],"metadata":{"id":"rqZQh8mUFbNZ","executionInfo":{"status":"ok","timestamp":1672555776941,"user_tz":-540,"elapsed":527,"user":{"displayName":"이승윤","userId":"02154129095399496427"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["NeuralNetwork의 인스턴스(instance)를 생성하고 이를 device에 옮겨주고, 구조(structure)를 출력해보자"],"metadata":{"id":"FFhZ1xHuF81b"}},{"cell_type":"code","source":["model = NeuralNetwork().to(device)\n","print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aVJwfCWnF8b2","executionInfo":{"status":"ok","timestamp":1672555848923,"user_tz":-540,"elapsed":4159,"user":{"displayName":"이승윤","userId":"02154129095399496427"}},"outputId":"92605390-66d3-45df-dad9-9bdf603d810c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"]}]},{"cell_type":"markdown","source":["- 모델을 사용하기 위해서 입력 데이터를 전달한다. 이는 일부 백그라운드 연산들과 함께 모델의 forward를 실행한다. model.forward()를 직접 호출하지 않아도 된다.\n","- 모델에 입력을 호출하면 각 class에 대한 원시(raw) 예측값이 있는 10차원 tensor가 반환된다. 이렇게 반환된 원시 예측값을 nn.Softmax 모듈의 인스턴스에 통과시켜 예측 확률을 얻게된다."],"metadata":{"id":"XXFBovzrGQlb"}},{"cell_type":"code","source":["X = torch.rand(1, 28, 28, device=device)\n","logits = model(X)\n","pred_probab = nn.Softmax(dim=1)(logits)\n","y_pred = pred_probab.argmax(1)\n","print(f\"Predicted class : {y_pred} | 예측 클래스는 {y_pred}입니다!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fk6p6UluGM5l","executionInfo":{"status":"ok","timestamp":1672556020085,"user_tz":-540,"elapsed":2966,"user":{"displayName":"이승윤","userId":"02154129095399496427"}},"outputId":"577c8642-13f8-48e9-feeb-50753e30086c"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted class : tensor([6], device='cuda:0') | 예측 클래스는 tensor([6], device='cuda:0')입니다!\n"]}]},{"cell_type":"markdown","source":["모델 계층(Layer)\n","- FashionMNIST 모델의 계층들을 살펴보자. 이를 보기 위해 28x28 크기의 이미지 3개로 구성된 미니 배치를 가져와서 신경망을 통과할 때 어떤 일이 일어나는지 알아보자."],"metadata":{"id":"tJFwz1RmHPBt"}},{"cell_type":"code","source":["input_image = torch.rand(3, 28, 28)\n","print(input_image.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJ20vKxpG3GM","executionInfo":{"status":"ok","timestamp":1672556175155,"user_tz":-540,"elapsed":4,"user":{"displayName":"이승윤","userId":"02154129095399496427"}},"outputId":"dc006b7d-9741-4c1f-acf6-c158a8595f93"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 28, 28])\n"]}]},{"cell_type":"markdown","source":["nn.Flatten 계층을 초기화해서 각 2D 이미지를 784픽셀(28x28) 값을 갖는 연속된 배열로 변환시켜보자. 이 때, dim=0의 미니 배치의 차원수는 유지된다."],"metadata":{"id":"XCd7nsc4Hfpc"}},{"cell_type":"code","source":["flatten = nn.Flatten()\n","flat_image = flatten(input_image)\n","print(flat_image.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ysrNvVJwHdwt","executionInfo":{"status":"ok","timestamp":1672556256081,"user_tz":-540,"elapsed":4,"user":{"displayName":"이승윤","userId":"02154129095399496427"}},"outputId":"f168ef52-5d8b-49a8-f74e-0f7f09c4a114"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 784])\n"]}]},{"cell_type":"markdown","source":["nn.Linear 계층은 저장된 가중치(weight)와 편향(bias)을 사용해서 입력 데이터에 선형 변환(linear transformation)을 적용하는 모듈이다."],"metadata":{"id":"F4U25MY7HzO0"}},{"cell_type":"code","source":["layer1 = nn.Linear(in_features=28*28, out_features=20)\n","hidden1 = layer1(flat_image)\n","print(hidden1.size())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6L7ljmcJHxa1","executionInfo":{"status":"ok","timestamp":1672556349057,"user_tz":-540,"elapsed":4,"user":{"displayName":"이승윤","userId":"02154129095399496427"}},"outputId":"ad6fe2c3-3a74-4cb1-f4e0-9ce58331bad2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 20])\n"]}]},{"cell_type":"markdown","source":["- 비선형 활성화(activation)는 모델의 입력과 출력 사이에 복잡한 관계(mapping)를 만든다. 비선형 활성화는 선형 변환 후에 적용되어 비선형성(non-linearity)을 도입하고, 신경망이 다양한 현상을 학습할 수 있도록 돕는다.\n","- 이 모델에서는 nn.ReLU를 선형 계층들 사이에 사용하지만, 모델을 만들 때는 비선형성을 가진 다른 활성화 함수를 도입할 수도 있다."],"metadata":{"id":"DlKuEkz8ITR6"}},{"cell_type":"code","source":["print(f\"Before ReLU : {hidden1}\\n\\n\")\n","hidden1 = nn.ReLU()(hidden1)\n","print(f\"After ReLU : {hidden1}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XowAa1FFIIHU","executionInfo":{"status":"ok","timestamp":1672556500285,"user_tz":-540,"elapsed":4,"user":{"displayName":"이승윤","userId":"02154129095399496427"}},"outputId":"49c5e4df-de3b-4ff4-b4f0-3515a76fc428"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Before ReLU : tensor([[ 0.2199, -0.1440,  0.3022,  0.5243, -0.5172, -0.2134, -0.5256,  0.3203,\n","          0.3196, -0.3266,  0.0295,  0.0347,  0.3861, -0.3538, -0.3041,  0.3429,\n","          0.7189,  0.1225, -0.0743, -0.2830],\n","        [-0.2376, -0.0986,  0.3822,  0.7964, -0.4125, -0.1953, -0.0589,  0.4395,\n","          0.1655,  0.1001,  0.4564,  0.2645,  0.0458, -0.3512, -0.0416,  0.1713,\n","          0.5270,  0.1643, -0.0144, -0.0137],\n","        [-0.1336,  0.0295,  0.0982,  0.9017, -0.2945, -0.0614, -0.0502,  0.2488,\n","          0.3154, -0.0222,  0.5005,  0.5749, -0.1762, -0.5271, -0.0622,  0.3122,\n","          0.4012, -0.2462, -0.2006, -0.2985]], grad_fn=<AddmmBackward0>)\n","\n","\n","After ReLU : tensor([[0.2199, 0.0000, 0.3022, 0.5243, 0.0000, 0.0000, 0.0000, 0.3203, 0.3196,\n","         0.0000, 0.0295, 0.0347, 0.3861, 0.0000, 0.0000, 0.3429, 0.7189, 0.1225,\n","         0.0000, 0.0000],\n","        [0.0000, 0.0000, 0.3822, 0.7964, 0.0000, 0.0000, 0.0000, 0.4395, 0.1655,\n","         0.1001, 0.4564, 0.2645, 0.0458, 0.0000, 0.0000, 0.1713, 0.5270, 0.1643,\n","         0.0000, 0.0000],\n","        [0.0000, 0.0295, 0.0982, 0.9017, 0.0000, 0.0000, 0.0000, 0.2488, 0.3154,\n","         0.0000, 0.5005, 0.5749, 0.0000, 0.0000, 0.0000, 0.3122, 0.4012, 0.0000,\n","         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"]}]},{"cell_type":"markdown","source":["nn.Sequential은 순서를 갖는 모듈의 컨테이너라고 볼 수 있다. 데이터는 정의된 것과 같은 순서로 모든 모듈들을 통해 전달된다. 순차 컨테이너(sequential container)를 사용해서 아래의 seq_modules와 같은 신경망을 만들어낼 수 있다."],"metadata":{"id":"XS5iZ3HNIz-w"}},{"cell_type":"code","source":["seq_modules = nn.Sequential(\n","    flatten,\n","    layer1,\n","    nn.ReLU(),\n","    nn.Linear(20,10)\n",")\n","input_image = torch.rand(3,28,28)\n","logits = seq_modules(input_image)"],"metadata":{"id":"AR4H8UAmIs_j","executionInfo":{"status":"ok","timestamp":1672556634559,"user_tz":-540,"elapsed":3,"user":{"displayName":"이승윤","userId":"02154129095399496427"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["- 신경망의 마지막 선형 계층은 nn.Softmax 모듈에 전달될 [-infty, infty] 범위의 원시 값(raw value)인 logits를 반환한다.\n","- logits는 모델의 각 class에 대한 예측 확률을 나타내도록 [0,1] 범위로 비례해서 조정된다.\n","- dim 매개변수는 값의 합이 1이 되는 차원을 나타낸다."],"metadata":{"id":"cCltr74nJQJL"}},{"cell_type":"code","source":["softmax = nn.Softmax(dim=1)\n","pred_probab= softmax(logits)"],"metadata":{"id":"mcXUd_02JN79","executionInfo":{"status":"ok","timestamp":1672556761632,"user_tz":-540,"elapsed":2,"user":{"displayName":"이승윤","userId":"02154129095399496427"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["모델 매개변수\n","- 신경망 내부의 많은 계층들은 매개변수화(parameterize) 된다. 즉, 학습 중에 최적화되는 가중치와 편향과 연관지어진다. nn.Module을 상속하면 모델 객체 내부의 모든 필드들이 자동으로 추적되며, 모델의 parameters() 및 named_parameters() 메소드로 모든 매개변수에 접근할 수 있게 된다.\n","- 본 예제에서는 각 매개변수들을 순회(iterate)하며, 매개변수의 크기와 값을 출력해보자."],"metadata":{"id":"dzvaUm3-JuY4"}},{"cell_type":"code","source":["print(f\"Model structure : {model}\\n\\n\")\n","for name, param in model.named_parameters() :\n","    print(f\"Layer : {name} | Size : {param.size()} | Values : {param[:2]} \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rePqDmNCJs3Z","executionInfo":{"status":"ok","timestamp":1672556988936,"user_tz":-540,"elapsed":360,"user":{"displayName":"이승윤","userId":"02154129095399496427"}},"outputId":"754b0062-e1be-4d85-ed99-75da77875e34"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Model structure : NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n","\n","\n","Layer : linear_relu_stack.0.weight | Size : torch.Size([512, 784]) | Values : tensor([[-0.0329,  0.0067,  0.0260,  ..., -0.0318,  0.0129,  0.0164],\n","        [ 0.0156, -0.0286,  0.0066,  ...,  0.0214, -0.0209, -0.0329]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer : linear_relu_stack.0.bias | Size : torch.Size([512]) | Values : tensor([-0.0252, -0.0303], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer : linear_relu_stack.2.weight | Size : torch.Size([512, 512]) | Values : tensor([[ 0.0232, -0.0239, -0.0217,  ..., -0.0344, -0.0239, -0.0110],\n","        [-0.0037,  0.0278, -0.0331,  ...,  0.0209, -0.0100, -0.0280]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer : linear_relu_stack.2.bias | Size : torch.Size([512]) | Values : tensor([-0.0246, -0.0068], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer : linear_relu_stack.4.weight | Size : torch.Size([10, 512]) | Values : tensor([[-0.0179,  0.0416,  0.0422,  ...,  0.0391, -0.0184, -0.0057],\n","        [ 0.0001,  0.0368, -0.0367,  ...,  0.0169, -0.0304, -0.0305]],\n","       device='cuda:0', grad_fn=<SliceBackward0>) \n","\n","Layer : linear_relu_stack.4.bias | Size : torch.Size([10]) | Values : tensor([0.0078, 0.0338], device='cuda:0', grad_fn=<SliceBackward0>) \n","\n"]}]}]}