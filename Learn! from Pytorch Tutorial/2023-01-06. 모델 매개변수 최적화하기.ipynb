{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlXWCLLdVRH/OblcNigZwt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 모델 매개변수 최적화하기\n","- URL from : https://tutorials.pytorch.kr/beginner/basics/optimization_tutorial.html\n","- 모델을 학습하는 과정은 반복을 거친다. epoch이라고 부르는 각 반복 단계에서 모델은 출력값을 추측하고, 추측한 값과 정답  사이의 오류(손실=loss)를 계산하고, 매개변수에 대한 오류의 도함수(derivative)를 수집한 뒤, 경사하강법을 사용해서 이 파라미터들을 최적화(optimize)한다."],"metadata":{"id":"WM2M_u8BcgCW"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"_F2QPsPjcb_8","executionInfo":{"status":"ok","timestamp":1672934209579,"user_tz":-540,"elapsed":1106,"user":{"displayName":"이승윤","userId":"02154129095399496427"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor, Lambda\n","\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor()\n",")\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor()\n",")\n","\n","train_dataloader = DataLoader(training_data, batch_size=64)\n","test_dataloader = DataLoader(test_data, batch_size=64)\n","\n","class NeuralNetwork(nn.Module) :\n","    def __init__(self) :\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        self.linear_relu_stack = nn.Sequential(\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10)\n","        )\n","    \n","    def forward(self, x) :\n","        x = self.flatten(x)\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n","model = NeuralNetwork()"]},{"cell_type":"markdown","source":["하이퍼파라미터(Hyperparameter)\n","- 하이퍼파라미터는 모델 최적화 과정을 제어할 수 있는 조절 가능한 매개변수를 말한다. 서로 다른 하이퍼파라미터 값은 모델 학습과 수렴률(convergence rate)에 영향을 미칠 수 있다.\n","- 학습 시에는 다음과 같은 하이퍼파라미터를 정의한다.\n","    - epoch : 데이터셋을 반복하는 횟수\n","    - batch size : 매개변수가 갱신되기 전 신경망을 통해 전파된 데이터 샘플의 수\n","    - learning rate : 각 batch/epoch에서 모델의 매개변수를 조절하는 비율로, 값이 작을수록 학습 속도가 느려지고, 값이 크면 학습 중 예측할 수 없는 동작이 발생할 수 있다."],"metadata":{"id":"449dLc92gbv9"}},{"cell_type":"code","source":["epochs = 5\n","batch_size = 128\n","learning_rate = 1e-3"],"metadata":{"id":"GTAy1fxfgSuM","executionInfo":{"status":"ok","timestamp":1672934209580,"user_tz":-540,"elapsed":12,"user":{"displayName":"이승윤","userId":"02154129095399496427"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["최적화 단계(Optimization Loop)\n","- 하이퍼파라미터를 설정한 뒤에는 최적화 단계를 통해 모델을 학습하고 최적화할 수 있다. 최적화 단계의 각 반복(iteration)을 epoch이라고 부른다.\n","- 하나의 epoch은 다음의 두 부분으로 구성된다.\n","    - 학습 단계(train loop) : 학습용 데이터셋을 반복(iterate)하고 최적의 매개변수로 수렴한다.\n","    - 검증/시험 단계(validation/test loop) : 모델 성능이 개선되고 있는지를 확인하기 위해 테스트 데이터셋을 반복(iterate)한다."],"metadata":{"id":"KT6Nt1MDg-2W"}},{"cell_type":"markdown","source":["손실 함수(Loss Function)\n","- 학습용 데이터를 제공하면, 학습되지 않은 신경망은 정답을 제공하지 않을 확률이 높다.\n","- 손실 함수는 획득한 결과와 실제 값 사이의 틀린 정도(degree of dissimilarity)를 측정하며, 학습 중에 이 값을 최소화하려고 한다. 주어진 데이터 샘플을 입력으로 계산한 예측과 정답(label)을 비교해서 손실(loss)를 계산한다.\n","- 일반적인 손실함수에는 회귀 문제(regression task)에 사용하는 nn.MSELoss(평균제곱오차, mean square error)나 분류(classification) 문제에 사용하는 nn.NLLLoss(음의 로그 우도, negative log likelihood), 그리고 nn.LogSoftmax와 nn.NLLLoss를 합친 nn.CrossEntropyLoss 등이 있다.\n","- 모델의 출력 로짓(logit)을 nn.CrossEntropyLoss에 전달하여 로짓(logit)을 정규화하고 예측 오류를 계산해보자"],"metadata":{"id":"XqV6d1ZQha2N"}},{"cell_type":"code","source":["# 손실 함수를 초기화한다.\n","loss_fn = nn.CrossEntropyLoss()"],"metadata":{"id":"2GAxtC8PhZA6","executionInfo":{"status":"ok","timestamp":1672934209580,"user_tz":-540,"elapsed":11,"user":{"displayName":"이승윤","userId":"02154129095399496427"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["옵티마이저(optimizer)\n","- 최적화는 각 학습 단계에서 모델의 오류를 줄이기 위해 모델 매개변수를 조정하는 과정이다. 최적화 알고리즘은 이 과정이 수행되는 방식을 정의하는데, 본 문서에서는 확률적 경사하강법(SGD, stochastic gradient descent)을 사용할 것이다.\n","- 모든 최적화 절차(logic)는 optimizer 객체에 캡슐화(encapsulate)된다.\n","- 학습하려는 모델의 매개변수와 학습률 하이퍼파라미터를 등록해서 옵티마이저를 초기화한다."],"metadata":{"id":"1nkoDiPMinQk"}},{"cell_type":"code","source":["optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"],"metadata":{"id":"-fvj26dckG7k","executionInfo":{"status":"ok","timestamp":1672934209581,"user_tz":-540,"elapsed":11,"user":{"displayName":"이승윤","userId":"02154129095399496427"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["학습 단계에서 최적화는 3단계로 이루어진다.\n","- optimizer.zero_grad()를 호출해서 모델 매개변수의 변화도를 재설정한다. 기본적으로 변화도는 더해지기 때문에 중복 계산을 막기 위해 반복할 때마다 명시적으로 0으로 설정해야 한다.\n","- loss.backwards()를 호출해서 예측 손실(prediction loss)를 역전파한다. 각 매개변수에 대한 손실의 변화도를 저장한다.\n","- 변화도를 계산한 뒤에는 optimizer.step()을 호출해서 역전파 단계에서 수집된 변화도로 매개변수를 조정한다."],"metadata":{"id":"JSoiSG_5kUf5"}},{"cell_type":"markdown","source":["최적화 코드를 반복하여 수행하는 train_loop와 테스트 데이터로 모델의 성능을 측정하는 test_loop를 정의한다."],"metadata":{"id":"xg1a69j9mIpe"}},{"cell_type":"code","source":["def train_loop(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    for batch, (X, y) in enumerate(dataloader):\n","        # 예측(prediction)과 손실(loss) 계산\n","        pred = model(X)\n","        loss = loss_fn(pred, y)\n","\n","        # 역전파\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        if batch % 100 == 0:\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","\n","def test_loop(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)\n","    num_batches = len(dataloader)\n","    test_loss, correct = 0, 0\n","\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    test_loss /= num_batches\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"],"metadata":{"id":"MP8Ss2VLkLYN","executionInfo":{"status":"ok","timestamp":1672934209581,"user_tz":-540,"elapsed":11,"user":{"displayName":"이승윤","userId":"02154129095399496427"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["손실 함수와 옵티마이저를 초기화하고 train_loop와 test_loop에 전달한다. 모델 성능 향상을 알아보기 위해 epoch수를 증가시켜볼 수도 있다."],"metadata":{"id":"E1APHccgn47u"}},{"cell_type":"code","source":["loss_fn = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","\n","epochs = 10\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(train_dataloader, model, loss_fn, optimizer)\n","    test_loop(test_dataloader, model, loss_fn)\n","print(\"끝!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aabb3Jyvn-yj","executionInfo":{"status":"ok","timestamp":1672934311508,"user_tz":-540,"elapsed":101938,"user":{"displayName":"이승윤","userId":"02154129095399496427"}},"outputId":"dac3ce35-50e8-481e-d6da-d54e30b4954e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1\n","-------------------------------\n","loss: 2.317406  [    0/60000]\n","loss: 2.298738  [ 6400/60000]\n","loss: 2.281718  [12800/60000]\n","loss: 2.271687  [19200/60000]\n","loss: 2.253535  [25600/60000]\n","loss: 2.232462  [32000/60000]\n","loss: 2.230743  [38400/60000]\n","loss: 2.203840  [44800/60000]\n","loss: 2.192293  [51200/60000]\n","loss: 2.159283  [57600/60000]\n","Test Error: \n"," Accuracy: 46.9%, Avg loss: 2.154099 \n","\n","Epoch 2\n","-------------------------------\n","loss: 2.164813  [    0/60000]\n","loss: 2.153424  [ 6400/60000]\n","loss: 2.094896  [12800/60000]\n","loss: 2.109108  [19200/60000]\n","loss: 2.051827  [25600/60000]\n","loss: 2.001537  [32000/60000]\n","loss: 2.017598  [38400/60000]\n","loss: 1.941753  [44800/60000]\n","loss: 1.937345  [51200/60000]\n","loss: 1.866813  [57600/60000]\n","Test Error: \n"," Accuracy: 57.0%, Avg loss: 1.868035 \n","\n","Epoch 3\n","-------------------------------\n","loss: 1.898900  [    0/60000]\n","loss: 1.871711  [ 6400/60000]\n","loss: 1.748349  [12800/60000]\n","loss: 1.793419  [19200/60000]\n","loss: 1.683150  [25600/60000]\n","loss: 1.644498  [32000/60000]\n","loss: 1.660626  [38400/60000]\n","loss: 1.569491  [44800/60000]\n","loss: 1.591342  [51200/60000]\n","loss: 1.491931  [57600/60000]\n","Test Error: \n"," Accuracy: 60.3%, Avg loss: 1.512742 \n","\n","Epoch 4\n","-------------------------------\n","loss: 1.571092  [    0/60000]\n","loss: 1.549206  [ 6400/60000]\n","loss: 1.392006  [12800/60000]\n","loss: 1.470458  [19200/60000]\n","loss: 1.360754  [25600/60000]\n","loss: 1.358173  [32000/60000]\n","loss: 1.369987  [38400/60000]\n","loss: 1.301879  [44800/60000]\n","loss: 1.336856  [51200/60000]\n","loss: 1.240933  [57600/60000]\n","Test Error: \n"," Accuracy: 62.5%, Avg loss: 1.266481 \n","\n","Epoch 5\n","-------------------------------\n","loss: 1.332500  [    0/60000]\n","loss: 1.329043  [ 6400/60000]\n","loss: 1.153852  [12800/60000]\n","loss: 1.262586  [19200/60000]\n","loss: 1.149053  [25600/60000]\n","loss: 1.168444  [32000/60000]\n","loss: 1.187657  [38400/60000]\n","loss: 1.131761  [44800/60000]\n","loss: 1.172994  [51200/60000]\n","loss: 1.088282  [57600/60000]\n","Test Error: \n"," Accuracy: 64.4%, Avg loss: 1.108531 \n","\n","Epoch 6\n","-------------------------------\n","loss: 1.168432  [    0/60000]\n","loss: 1.185436  [ 6400/60000]\n","loss: 0.993176  [12800/60000]\n","loss: 1.128334  [19200/60000]\n","loss: 1.012615  [25600/60000]\n","loss: 1.035948  [32000/60000]\n","loss: 1.070653  [38400/60000]\n","loss: 1.019262  [44800/60000]\n","loss: 1.060861  [51200/60000]\n","loss: 0.989109  [57600/60000]\n","Test Error: \n"," Accuracy: 65.6%, Avg loss: 1.002538 \n","\n","Epoch 7\n","-------------------------------\n","loss: 1.049786  [    0/60000]\n","loss: 1.089061  [ 6400/60000]\n","loss: 0.879446  [12800/60000]\n","loss: 1.036016  [19200/60000]\n","loss: 0.923330  [25600/60000]\n","loss: 0.939642  [32000/60000]\n","loss: 0.991846  [38400/60000]\n","loss: 0.943485  [44800/60000]\n","loss: 0.980926  [51200/60000]\n","loss: 0.920861  [57600/60000]\n","Test Error: \n"," Accuracy: 66.9%, Avg loss: 0.928135 \n","\n","Epoch 8\n","-------------------------------\n","loss: 0.960625  [    0/60000]\n","loss: 1.020072  [ 6400/60000]\n","loss: 0.795793  [12800/60000]\n","loss: 0.968846  [19200/60000]\n","loss: 0.861987  [25600/60000]\n","loss: 0.867333  [32000/60000]\n","loss: 0.935327  [38400/60000]\n","loss: 0.890886  [44800/60000]\n","loss: 0.922003  [51200/60000]\n","loss: 0.870589  [57600/60000]\n","Test Error: \n"," Accuracy: 68.3%, Avg loss: 0.873191 \n","\n","Epoch 9\n","-------------------------------\n","loss: 0.891248  [    0/60000]\n","loss: 0.966832  [ 6400/60000]\n","loss: 0.732020  [12800/60000]\n","loss: 0.917747  [19200/60000]\n","loss: 0.817128  [25600/60000]\n","loss: 0.812034  [32000/60000]\n","loss: 0.892091  [38400/60000]\n","loss: 0.853308  [44800/60000]\n","loss: 0.877531  [51200/60000]\n","loss: 0.831222  [57600/60000]\n","Test Error: \n"," Accuracy: 69.6%, Avg loss: 0.830906 \n","\n","Epoch 10\n","-------------------------------\n","loss: 0.835460  [    0/60000]\n","loss: 0.923283  [ 6400/60000]\n","loss: 0.681947  [12800/60000]\n","loss: 0.877676  [19200/60000]\n","loss: 0.782548  [25600/60000]\n","loss: 0.769003  [32000/60000]\n","loss: 0.857103  [38400/60000]\n","loss: 0.825269  [44800/60000]\n","loss: 0.843153  [51200/60000]\n","loss: 0.799244  [57600/60000]\n","Test Error: \n"," Accuracy: 71.0%, Avg loss: 0.797145 \n","\n","끝!\n"]}]}]}